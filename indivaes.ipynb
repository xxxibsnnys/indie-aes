{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2667,"databundleVersionId":29898,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unidecode\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:44:08.902824Z","iopub.execute_input":"2024-05-20T02:44:08.903173Z","iopub.status.idle":"2024-05-20T02:44:35.384173Z","shell.execute_reply.started":"2024-05-20T02:44:08.903145Z","shell.execute_reply":"2024-05-20T02:44:35.382981Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting unidecode\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.8\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nimport sys\nimport argparse\n\nfrom transformers import DataCollatorWithPadding, TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\nimport evaluate\nimport torch\nfrom datasets import Dataset, DatasetDict\n\nimport numpy as np\nimport pandas as pd\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport json\nimport collections\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import *\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T02:44:35.386696Z","iopub.execute_input":"2024-05-20T02:44:35.387108Z","iopub.status.idle":"2024-05-20T02:44:53.729622Z","shell.execute_reply.started":"2024-05-20T02:44:35.387068Z","shell.execute_reply":"2024-05-20T02:44:53.728687Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-20 02:44:43.226195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-20 02:44:43.226298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-20 02:44:43.360824: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Util","metadata":{}},{"cell_type":"code","source":"augment_set = ['no_art', 'no_conj', 'add_and-0.1', 'swap_word-0.05',\n               'no_first_sent', 'no_last_sent', 'no_longest_sent', 'reverse_sent']\n\n\nMAXLEN = [-1, 70, 88, 22, 23, 24, 20, 67, 97]\nMAXWORDLEN = 50\n\nPAD_SENT_TOKEN = ''\n\nscore_range = [(-1, -1),\n               (2, 12),\n               (1, 6),\n               (0, 3),\n               (0, 3),\n               (0, 4),\n               (0, 4),\n               (0, 30),\n               (0, 60)]\n\n\ndef get_threshold(p):\n    low, high = score_range[p]\n    return 1/((high - low))\n\n\ndef rescale_to_int(raw, p):\n    low, high = score_range[p]\n    return np.around(raw*(high-low)+low).astype(int)\n\ndef normalize_score(Y, p):\n    low, high = score_range[p]\n    return (Y-low)/(high-low)\n\n\ndef clean_data(df):\n    new_df = []\n    for essay in df:\n        new_df.append(clean_text(essay))\n    return new_df\n\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove quotation\n    text = re.sub(r'\\\"', '', text)\n    # URL replace by https://github.com/feidong1991/aes\n    text = re.sub(\n        '(http[s]?://)?((www)\\.)?([a-zA-Z0-9]+)\\.{1}((com)(\\.(cn))?|(org))', '<url>', text)\n    # Truncate any duplicate non-alphanumeric and add a space after it\n    # e.g. sent1.sent2!!!...??? becomes sent1. sent2! . ?\n    text = re.sub(r'([^a-zA-Z0-9_@\\'\\s])\\1*', r'\\1 ', text)\n\n    # Remove extra whitespaces\n    text = re.sub(r'\\s+', ' ', text)\n\n    return text\n\n\ndef mkpath(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path + '/'\n\n\ndef shorten_sentence(tokens):\n    if len(tokens) <= MAXWORDLEN:\n        return [tokens]\n\n    # Step 1: split sentence based on keywords\n    # split_keywords = ['because', 'but', 'so', 'then', 'You', 'He', 'She', 'We', 'It', 'They', 'Your', 'His', 'Her']\n    split_keywords = ['because', 'but', 'so', 'then']\n    k_indexes = [i for i, key in enumerate(tokens) if key in split_keywords]\n    processed_tokens = []\n    if not k_indexes:\n        num = len(tokens) // MAXWORDLEN\n        k_indexes = [(i+1)*MAXWORDLEN for i in range(num)]\n\n    if len(tokens[:k_indexes[0]]) > 0:\n        processed_tokens.append(tokens[:k_indexes[0]])\n    len_k = len(k_indexes)\n    for j in range(len_k-1):\n        processed_tokens.append(tokens[k_indexes[j]:k_indexes[j+1]])\n    processed_tokens.append(tokens[k_indexes[-1]:])\n\n    # Step 2: split sentence to no more than MAXWORDLEN\n    # if there are still sentences whose length exceeds MAXWORDLEN\n    new_tokens = []\n    for token in processed_tokens:\n        if len(token) > MAXWORDLEN:\n            num = len(token) // MAXWORDLEN\n            s_indexes = [(i+1)*MAXWORDLEN for i in range(num)]\n            len_s = len(s_indexes)\n            if len(token[:s_indexes[0]]) > 0:\n                new_tokens.append(token[0:s_indexes[0]])\n            for j in range(len_s-1):\n                new_tokens.append(token[s_indexes[j]:s_indexes[j+1]])\n            new_tokens.append(token[s_indexes[-1]:])\n        else:\n            new_tokens.append(token)\n    # print('before', len(tokens), 'after', [len(x) for x in new_tokens])\n    return new_tokens\n\n\ndef get_vocab(prompt, df=None, length=4000, features='essay'):\n    vocab_path = mkpath('vocab')\n    file_path = os.path.join(vocab_path, '{}.vocab'.format(prompt))\n    if os.path.isfile(file_path):\n        with open(file_path, 'r') as f:\n            vocab = json.load(f)\n        assert type(vocab) == dict\n        print('load vocab from {}'.format(file_path))\n        return vocab\n\n    word_all = []\n    for essay in df[features]:\n        sents = sentenize(essay)\n        for sent in sents:\n            words = tokenize(sent)\n            word_all.extend(words)\n    print('word count:', len(word_all))\n    print('unique word count:', len(set(word_all)))\n\n    most_common = collections.Counter(word_all).most_common(length - 3)\n\n    vocab = {'<pad>': 0, '<unk>': 1, '<num>': 2}\n    for w, c in most_common:\n        vocab[w] = len(vocab)\n\n    # save as JSON\n    with open(file_path, 'w') as f:\n        json.dump(vocab, f)\n    print('save vocab to {}'.format(file_path))\n\n    return vocab\n\n\ndef word2idx(w, vocab):\n    if not w in vocab:\n        return vocab['<unk>']\n    return vocab[w]\n\n\ndef load_data(prompt, suffix=None):\n    if suffix:\n        data = pd.read_csv(\n            'asap/prompt_{}_{}.tsv'.format( prompt, suffix), sep='\\t')\n    else:\n        data = pd.read_csv(\n            'asap/prompt_{}.tsv'.format( prompt), sep='\\t')\n    return data\n\n\ndef prepare_features(model_name, **kwargs):\n    if model_name.startswith('elmo'):\n        return prepare_elmo_features(**kwargs)\n    elif model_name.startswith('glove'):\n        return prepare_glove_features(**kwargs)\n\n\ndef gen(model_name, prompt, df, vocab=None, batch_size=1, test=False, shuffle=True, **kwargs):\n    data = df.copy()\n    while True:\n        if shuffle:\n            data = data.sample(frac=1).reset_index(drop=True)\n        for i in range(0, len(data), batch_size):\n            j = min(len(data), i+batch_size)\n            if test:\n                x = prepare_features(model_name,\n                                     df=data[i:j], prompt=prompt, vocab=vocab, x_only=True, **kwargs)\n                yield x\n            else:\n                x, y = prepare_features(model_name,\n                                        df=data[i:j], prompt=prompt, vocab=vocab, **kwargs)\n                yield x, y\n\n\ndef augment_gen(model_name, prompt, test_df, vocab=None, batch_size=1, augment=None, **kwargs):\n    data = test_df.copy()\n    rnd = np.random.RandomState(1)\n    while True:\n        for i in range(0, len(data), batch_size):\n            j = min(len(data), i+batch_size)\n            x = prepare_features(model_name,\n                                 df=data[i:j], prompt=prompt, vocab=vocab, x_only=True, augment=augment, rnd=rnd, **kwargs)\n            yield x\n\n\ndef make_augment(sents, augment, rnd=None):\n    '''augment essay (list of sentences)'''\n    assert augment in augment_set\n    t = augment.split('-')\n    if len(t) > 1:\n        augment, threshold = t[0], float(t[1])\n    else:\n        threshold = 1.0\n\n    new_sents = []\n    if not rnd:\n        rnd = np.random.RandomState(1)\n\n    if augment == 'no_art':\n        for sent in sents:\n            new_sents.append(re.sub(r'\\b(a|an|the)\\b ', r'', sent))\n\n    elif augment == 'no_conj':\n        for sent in sents:\n            new_sents.append(re.sub(r'\\b(and|or|but)\\b ', r'', sent))\n\n    elif augment == 'add_and':\n        for sent in sents:\n            state = rnd.rand()\n            if state < threshold:\n                sent = 'and ' + sent\n            new_sents.append(sent)\n\n    elif augment == 'swap_word':\n        for sent in sents:\n            words = sent.split()\n            word_idx = np.arange(len(words)-2)\n            rnd.shuffle(word_idx)\n            for i in word_idx:\n                state = rnd.rand()\n                if state < threshold:\n                    words[i], words[i+1] = words[i+1], words[i]\n            new_sents.append(' '.join(words))\n\n    elif augment == 'no_first_sent':\n        if len(sents) > 1:\n            new_sents.extend(sents[1:])\n        else:\n            new_sents.extend(['.'])\n\n    elif augment == 'no_last_sent':\n        if len(sents) > 1:\n            new_sents.extend(sents[:-1])\n        else:\n            new_sents.extend(['.'])\n\n    elif augment == 'no_longest_sent':\n        if len(sents) > 1:\n            maxidx = np.argmax([len(sent) for sent in sents])\n            new_sents.extend(sents[:maxidx] + sents[maxidx+1:])\n        else:\n            new_sents.extend(['.'])\n\n    elif augment == 'reverse_sent':\n        new_sents.extend(sents[::-1])\n\n    else:\n        raise NameError('Unknown augment : ' + str(augment))\n    assert type(new_sents) is list\n    return new_sents\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:44:53.731000Z","iopub.execute_input":"2024-05-20T02:44:53.731874Z","iopub.status.idle":"2024-05-20T02:44:53.775129Z","shell.execute_reply.started":"2024-05-20T02:44:53.731838Z","shell.execute_reply":"2024-05-20T02:44:53.774014Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Util","metadata":{}},{"cell_type":"code","source":"class EvaluateCallback(Callback):\n    def __init__(self, prompt, val_data, model_name, vocab=None, batch_size=5):\n        self.prompt = prompt\n        self.val_data = val_data\n        self.model_name = model_name\n        self.vocab = vocab\n        self.batch_size = batch_size\n        self.steps = np.ceil(len(val_data) / batch_size)\n        self.y_true = prepare_features(model_name,\n                                       df=val_data, prompt=prompt, y_only=True)\n\n    def on_epoch_end(self, epoch, logs):\n        y_pred = self.model.predict_generator(\n            gen(self.model_name, self.prompt, self.val_data, self.vocab, self.batch_size, test=True, shuffle=False), steps=self.steps, verbose=1)\n\n        generate_qwk(self.prompt, self.model_name,\n                     self.y_true, y_pred, epoch+1, 'val')\n\n\ndef generate_qwk(prompt, model_name, y_true, y_pred, epoch, suffix=''):\n    path = mkpath('pred/{}'.format(model_name))\n\n    y_true = rescale_to_int(y_true, prompt)\n    y_pred = rescale_to_int(y_pred, prompt)\n    qwk = QWK(y_true, y_pred)\n\n    with open(os.path.join(path, 'qwk_{}_{}.csv'.format(prompt, suffix)), 'a+') as f:\n        f.write('{}, {}\\n'.format(epoch, qwk))\n\n\ndef generate_score(prompt, model_name, epoch, y_true, y_pred, aug_pred, test_df):\n    path = mkpath('pred/{}'.format(model_name))\n\n    df = pd.DataFrame()\n    df['essay_id'] = test_df['essay_id']\n    df['essay_set'] = test_df['essay_set']\n    df['domain1_score'] = y_true\n    df['test'] = y_pred\n    for key in aug_pred:\n        df['test_' + key] = aug_pred[key]\n    df.to_csv(os.path.join(path, 'score_{}_{}.tsv'.format(prompt, epoch)),\n              sep='\\t', index=False)\n    return df\n\n\ndef generate_robustness(prompt, model_name, epoch, y_true, y_pred, aug_pred):\n    path = mkpath('pred/{}'.format(model_name))\n\n    # y_true = rescale_to_int(y_true, prompt)\n    y_pred_int = rescale_to_int(y_pred, prompt)\n    aug_pred_int = {}\n    wr_t, br_t, w_t, b_t = 0, 0, 0, 0\n    N = len(y_pred) * len(aug_pred)\n    print('N :', N)\n\n    with open(os.path.join(path, 'robustness_{}_{}.csv'.format(prompt, epoch)), 'w+') as f:\n        f.write('augment,worse_raw,better_raw,worse_resolved,better_resolved\\n')\n        for key in aug_pred:\n            aug_pred_int[key] = rescale_to_int(aug_pred[key], prompt)\n\n            wr, br, w, b = robustness(\n                y_pred, aug_pred[key], y_pred_int, aug_pred_int[key])\n            wr_t += wr\n            br_t += br\n            w_t += w\n            b_t += b\n            f.write('{},{},{},{},{}\\n'.format(key, wr, br, w, b))\n        f.write('sum,{},{},{},{}\\n'.format(wr_t, br_t, w_t, b_t))\n        f.write('avg,{},{},{},{}\\n'.format(wr_t/N, br_t/N, w_t/N, b_t/N))\n\n\ndef generate_summary(model_name, epoch):\n    prompts = [1, 2, 3, 4, 5, 6, 7, 8]\n    # number of essay in test set\n    length = [-1, 179, 180, 173, 177, 181, 180, 157, 73]\n    path = mkpath('pred/{}'.format(model_name))\n\n    with open(os.path.join(path, 'summary_{}.txt'.format(epoch)), 'w+') as f:\n        f.write('{} epoch {}\\n\\n'.format(model_name, epoch))\n        f.write('QWK\\n')\n        qwk_avg = 0\n        for p in prompts:\n            qwk_df = pd.read_csv(os.path.join(path, 'qwk_{}_test.csv'.format(\n                p)), header=None, names=['epoch', 'qwk'])\n            qwk = qwk_df[qwk_df['epoch'] == epoch].values[-1, -1]\n            f.write('{}\\t{}\\n'.format(p, qwk))\n            qwk_avg += qwk\n\n        f.write('\\nRobustness per prompt\\n')\n        r_avg = 0\n        r_aug_avg = 0\n        for p in prompts:\n            robustness_df = pd.read_csv(os.path.join(\n                path, 'robustness_{}_{}.csv'.format(p, epoch)))\n            r = (robustness_df['worse_resolved'] -\n                 robustness_df['better_resolved']).values[-1]\n            f.write('{}\\t{}\\n'.format(p, r))\n            r_avg += r\n\n            r_aug = (robustness_df['worse_resolved'] -\n                     robustness_df['better_resolved']).values[:-2]/length[p]\n            r_aug_avg += r_aug\n\n        f.write('\\nRobustness per augment\\n')\n        r_aug_avg /= 8\n        for a, r in zip(robustness_df['augment'][:-2], r_aug_avg):\n            f.write('{}\\t{}\\n'.format(a, r))\n\n        f.write('\\n')\n        f.write('QWK Average:\\t{}\\n'.format(qwk_avg / 8))\n        f.write('Robustness Average:\\t{}\\n'.format(r_avg / 8))\n        f.write('Robustness Average:\\t{}\\n'.format(r_aug_avg.mean()))\n    print('summary generated!')\n\n\ndef generate_summary_best(model_name):\n    prompts = [1, 2, 3, 4, 5, 6, 7, 8]\n    # number of essay in test set\n    length = [-1, 179, 180, 173, 177, 181, 180, 157, 73]\n    path = mkpath('pred/{}'.format(model_name))\n\n    best_ep = [-1]*9\n    with open(os.path.join(path, 'summary_best.txt'), 'w+') as f:\n        f.write('{}\\n\\n'.format(model_name))\n        f.write('QWK\\n')\n        f.write('epoch\\tprompt\\tqwk\\n')\n        qwk_avg = 0\n        for p in prompts:\n            qwk_df = pd.read_csv(os.path.join(path, 'qwk_{}_val.csv'.format(\n                p)), header=None, names=['epoch', 'qwk'])\n            max_idx = qwk_df['qwk'].idxmax()\n            best_ep[p] = int(qwk_df.iloc[max_idx].values[0])\n\n            qwk_df = pd.read_csv(os.path.join(path, 'qwk_{}_test.csv'.format(\n                p)), header=None, names=['epoch', 'qwk'])\n\n            try:\n                tmp = qwk_df[qwk_df['epoch'] == best_ep[p]].values\n                # in case of multiple runs of same epoch, pick one with the best QWK\n                ep, qwk = tmp[tmp.argmax(axis=0)[-1]]\n            except:\n                raise Exception(\n                    'Error: epoch {} of prompt {} not found in test'.format(best_ep[p], p))\n\n            f.write('{}\\t{}\\t{}\\n'.format(best_ep[p], p, qwk))\n            qwk_avg += qwk\n\n        f.write('\\nRobustness per prompt\\n')\n        r_avg = 0\n        r_aug_avg = 0\n        for p in prompts:\n            robustness_df = pd.read_csv(os.path.join(\n                path, 'robustness_{}_{}.csv'.format(p, best_ep[p])))\n            r = (robustness_df['worse_resolved'] -\n                 robustness_df['better_resolved']).values[-1]\n            f.write('{}\\t{}\\n'.format(p, r))\n            r_avg += r\n\n            r_aug = (robustness_df['worse_resolved'] -\n                     robustness_df['better_resolved']).values[:-2]/length[p]\n            r_aug_avg += r_aug\n\n        f.write('\\nRobustness per augment\\n')\n        r_aug_avg /= 8\n        for a, r in zip(robustness_df['augment'][:-2], r_aug_avg):\n            f.write('{}\\t{}\\n'.format(a, r))\n\n        f.write('\\n')\n        f.write('QWK Average:\\t{}\\n'.format(qwk_avg / 8))\n        f.write('Robustness Average:\\t{}\\n'.format(r_avg / 8))\n        f.write('Robustness Average:\\t{}\\n'.format(r_aug_avg.mean()))\n    print('summary generated!')\n\n\ndef QWK(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n\ndef robustness(original, augment, original_int, augment_int, threshold=0.0):\n    worse_raw = np.sum(original - augment > threshold)\n    better_raw = np.sum(augment - original > threshold)\n    worse_resolved = np.sum(original_int > augment_int)\n    better_resolved = np.sum(original_int < augment_int)\n    return worse_raw, better_raw, worse_resolved, better_resolved\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:44:53.778281Z","iopub.execute_input":"2024-05-20T02:44:53.778718Z","iopub.status.idle":"2024-05-20T02:44:53.816715Z","shell.execute_reply.started":"2024-05-20T02:44:53.778686Z","shell.execute_reply":"2024-05-20T02:44:53.815870Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom unidecode import unidecode\nfrom sklearn.model_selection import KFold, train_test_split\n\n\ndef convert_to_ascii(df):\n    new_df = []\n    for essay in df:\n        new_df.append(unidecode(essay))\n    return new_df\n\n\ndef create_dataset(fold=True):\n    '''Run this function once to create train,val,test files for K folds'''\n    data_all = pd.read_csv('/kaggle/input/asap-aes/training_set_rel3.tsv',\n                           sep='\\t', encoding='latin1')\n    data_all['essay'] = convert_to_ascii(data_all['essay'])\n    data_all['essay'] = clean_data(data_all['essay'])\n\n    for p in range(1, 9):\n#         path = mkpath('asap/')\n        data_prompt = data_all[data_all['essay_set']\n                               == p].reset_index(drop=True)\n        print(data_prompt.head())\n#         data_prompt.to_csv(path + 'prompt_{}_all.tsv'.format(p),\n#                          sep='\\t', index=False)\n\n        if fold:\n            kf = KFold(n_splits=5, shuffle=True, random_state=420)\n            n = 1\n            for train_index, test_index in kf.split(data_prompt):\n                # print(\"TRAIN:\", train_index[:10], \"TEST:\", test_index[:10])\n                val_index = test_index[:len(test_index)//2]\n                test_index = test_index[len(test_index)//2:]\n                print(len(train_index), len(val_index), len(test_index))\n\n                fold_path = mkpath('asap/fold_{}/'.format(n))\n                data_prompt.loc[train_index].to_csv(\n                    fold_path + 'prompt_{}_train.tsv'.format(p), sep='\\t', index=False)\n                data_prompt.loc[val_index].to_csv(\n                    fold_path + 'prompt_{}_val.tsv'.format(p), sep='\\t', index=False)\n                data_prompt.loc[test_index].to_csv(\n                    fold_path + 'prompt_{}_test.tsv'.format(p), sep='\\t', index=False)\n                n += 1\n        else:\n            train, test = train_test_split(\n                data_prompt, test_size=0.2, random_state=420, shuffle=False)\n            val = test[:len(test)//2]\n            test = test[len(test)//2:]\n            path = mkpath('asap/')\n            print(len(train), len(val), len(test))\n            train.to_csv(path + 'prompt_{}_train.tsv'.format(p),\n                         sep='\\t', index=False)\n            val.to_csv(path + 'prompt_{}_val.tsv'.format(p),\n                       sep='\\t', index=False)\n            test.to_csv(path + 'prompt_{}_test.tsv'.format(p),\n                        sep='\\t', index=False)\n\n\nif __name__ == \"__main__\":\n    create_dataset(fold=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:44:53.817693Z","iopub.execute_input":"2024-05-20T02:44:53.817959Z","iopub.status.idle":"2024-05-20T02:45:06.471984Z","shell.execute_reply.started":"2024-05-20T02:44:53.817936Z","shell.execute_reply":"2024-05-20T02:45:06.471026Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   essay_id  essay_set                                              essay  \\\n0         1          1  dear local newspaper, i think effects computer...   \n1         2          1  dear @caps1 @caps2, i believe that using compu...   \n2         3          1  dear, @caps1 @caps2 @caps3 more and more peopl...   \n3         4          1  dear local newspaper, @caps1 i have found that...   \n4         5          1  dear @location1, i know having computers has a...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               4               4             NaN              8   \n1               5               4             NaN              9   \n2               4               3             NaN              7   \n3               5               5             NaN             10   \n4               4               4             NaN              8   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            NaN   \n1             NaN             NaN            NaN  ...            NaN   \n2             NaN             NaN            NaN  ...            NaN   \n3             NaN             NaN            NaN  ...            NaN   \n4             NaN             NaN            NaN  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1426 178 179\n   essay_id  essay_set                                              essay  \\\n0      2978          2  certain materials being removed from libraries...   \n1      2979          2  write a persuasive essay to a newspaper reflec...   \n2      2980          2  do you think that libraries should remove cert...   \n3      2981          2  in @date1's world, there are many things found...   \n4      2982          2  in life you have the 'offensive things'. the l...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               4               4             NaN              4   \n1               1               2             NaN              1   \n2               2               3             NaN              2   \n3               4               4             NaN              4   \n4               4               4             NaN              4   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             4.0             4.0            4.0  ...            NaN   \n1             1.0             2.0            1.0  ...            NaN   \n2             3.0             3.0            3.0  ...            NaN   \n3             4.0             4.0            4.0  ...            NaN   \n4             4.0             4.0            4.0  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1440 180 180\n   essay_id  essay_set                                              essay  \\\n0      5978          3  the features of the setting affect the cyclist...   \n1      5979          3  the features of the setting affected the cycli...   \n2      5980          3  everyone travels to unfamiliar places. sometim...   \n3      5981          3  i believe the features of the cyclist affected...   \n4      5982          3  the setting effects the cyclist because of the...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               1               1             NaN              1   \n1               2               2             NaN              2   \n2               1               1             NaN              1   \n3               1               1             NaN              1   \n4               2               2             NaN              2   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            NaN   \n1             NaN             NaN            NaN  ...            NaN   \n2             NaN             NaN            NaN  ...            NaN   \n3             NaN             NaN            NaN  ...            NaN   \n4             NaN             NaN            NaN  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1380 173 173\n   essay_id  essay_set                                              essay  \\\n0      8863          4  the author concludes the story with this becau...   \n1      8864          4  the narrater has that in with paragraph becuse...   \n2      8865          4  the author concludes the story with that passa...   \n3      8866          4  the author ended the story with this paragraph...   \n4      8867          4  the author concludes the story with this parag...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               0               0             NaN              0   \n1               0               0             NaN              0   \n2               3               2             NaN              3   \n3               1               2             NaN              2   \n4               2               2             NaN              2   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            NaN   \n1             NaN             NaN            NaN  ...            NaN   \n2             NaN             NaN            NaN  ...            NaN   \n3             NaN             NaN            NaN  ...            NaN   \n4             NaN             NaN            NaN  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1416 177 177\n   essay_id  essay_set                                              essay  \\\n0     11827          5  in this memoir of narciso rodriguez, @person3'...   \n1     11828          5  throughout the excerpt from home the blueprint...   \n2     11829          5  the mood the author created in the memoir is l...   \n3     11830          5  the mood created by the author is showing how ...   \n4     11831          5  the mood created in the memoir is happiness an...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               2               2             NaN              2   \n1               2               2             NaN              2   \n2               3               3             NaN              3   \n3               1               0             NaN              1   \n4               2               3             NaN              3   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            NaN   \n1             NaN             NaN            NaN  ...            NaN   \n2             NaN             NaN            NaN  ...            NaN   \n3             NaN             NaN            NaN  ...            NaN   \n4             NaN             NaN            NaN  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1444 180 181\n   essay_id  essay_set                                              essay  \\\n0     14834          6  there were many obstacles that the builders fa...   \n1     14835          6  him from the start, there would have been many...   \n2     14836          6  the builders of the empire state building face...   \n3     14837          6  in the passage the mooring mast by marcia amid...   \n4     14838          6  the builders of the empire state building face...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               2               2             NaN              2   \n1               3               3             NaN              3   \n2               3               4             NaN              4   \n3               1               1             NaN              1   \n4               3               3             NaN              3   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            NaN   \n1             NaN             NaN            NaN  ...            NaN   \n2             NaN             NaN            NaN  ...            NaN   \n3             NaN             NaN            NaN  ...            NaN   \n4             NaN             NaN            NaN  ...            NaN   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            NaN            NaN            NaN            NaN            NaN   \n1            NaN            NaN            NaN            NaN            NaN   \n2            NaN            NaN            NaN            NaN            NaN   \n3            NaN            NaN            NaN            NaN            NaN   \n4            NaN            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1440 180 180\n   essay_id  essay_set                                              essay  \\\n0     17834          7  patience is when your waiting . i was patience...   \n1     17836          7  i am not a patience person, like i cant sit in...   \n2     17837          7  one day i was at basketball practice and i was...   \n3     17838          7  i going to write about a time when i went to t...   \n4     17839          7  it can be very hard for somebody to be patient...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0               8               7             NaN             15   \n1               6               7             NaN             13   \n2               7               8             NaN             15   \n3               8               9             NaN             17   \n4               7               6             NaN             13   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            2.0   \n1             NaN             NaN            NaN  ...            2.0   \n2             NaN             NaN            NaN  ...            2.0   \n3             NaN             NaN            NaN  ...            2.0   \n4             NaN             NaN            NaN  ...            1.0   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            2.0            NaN            NaN            NaN            NaN   \n1            1.0            NaN            NaN            NaN            NaN   \n2            2.0            NaN            NaN            NaN            NaN   \n3            3.0            NaN            NaN            NaN            NaN   \n4            2.0            NaN            NaN            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            NaN            NaN            NaN            NaN  \n2            NaN            NaN            NaN            NaN  \n3            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n1255 157 157\n   essay_id  essay_set                                              essay  \\\n0     20716          8   a long time ago when i was in third grade i h...   \n1     20717          8   softball has to be one of the single most gre...   \n2     20718          8   some people like making people laugh, i love ...   \n3     20719          8   laughter @caps1 i hang out with my friends, t...   \n4     20721          8  well ima tell a story about the time i got @ca...   \n\n   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0              18              16             NaN             34   \n1              21              26            46.0             46   \n2              15              20            40.0             40   \n3              12              20            30.0             30   \n4              11              15             NaN             26   \n\n   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0             NaN             NaN            NaN  ...            4.0   \n1             NaN             NaN            NaN  ...            6.0   \n2             NaN             NaN            NaN  ...            5.0   \n3             NaN             NaN            NaN  ...            4.0   \n4             NaN             NaN            NaN  ...            3.0   \n\n   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n0            4.0            3.0            3.0            NaN            NaN   \n1            6.0            5.0            5.0            5.0            5.0   \n2            4.0            4.0            4.0            4.0            4.0   \n3            4.0            4.0            4.0            3.0            3.0   \n4            3.0            3.0            3.0            NaN            NaN   \n\n   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n0            NaN            NaN            NaN            NaN  \n1            5.0            5.0            5.0            4.0  \n2            4.0            4.0            4.0            4.0  \n3            3.0            3.0            3.0            3.0  \n4            NaN            NaN            NaN            NaN  \n\n[5 rows x 28 columns]\n578 72 73\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SET UP","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 2e-5\nMAX_LENGTH = 256\nBATCH_SIZE = 8\nEPOCHS = 5\n\nprompts = [1, 2, 3, 4, 5, 6, 7, 8]\n\n\ndef process(df,p):\n    new_df = df[[\"essay\", \"domain1_score\"]].copy()\n    new_df[\"label\"] = new_df[\"domain1_score\"].apply(lambda x: normalize_score(x, p))\n    new_df.rename(columns={\"essay\": \"text\"}, inplace=True)\n    return new_df\n\ndef compute_metrics_for_regression(eval_pred):\n    logits, labels = eval_pred\n    logits = logits[:, 0]\n    mse = mean_squared_error(labels, logits)\n#     qwk = QWK(labels, logits)\n\n    return {\"mse\": mse}\n    \nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs[0][:, 0]\n        loss = torch.nn.functional.mse_loss(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n    \n# def quadratic_weighted_kappa(y_true, y_pred):\n#     \"\"\"\n#     Compute the quadratic weighted kappa (QWK) score between two arrays of ratings.\n#     \"\"\"\n#     assert len(y_true) == len(y_pred)\n#     min_rating, max_rating = score_range(p)\n    \n#     y_true = np.array(y_true, dtype=int)\n#     y_pred = np.array(y_pred, dtype=int)\n\n#     # Create the confusion matrix\n#     O = np.zeros((max_rating - min_rating + 1, max_rating - min_rating + 1))\n#     for a, p in zip(y_true, y_pred):\n#         O[a - min_rating, p - min_rating] += 1\n\n#     # Create the weight matrix\n#     W = np.zeros((max_rating - min_rating + 1, max_rating - min_rating + 1))\n#     for i in range(max_rating - min_rating + 1):\n#         for j in range(max_rating - min_rating + 1):\n#             W[i, j] = ((i - j) ** 2) / ((max_rating - min_rating) ** 2)\n\n#     # Calculate the expected matrix\n#     act_hist = np.bincount(y_true - min_rating, minlength=max_rating - min_rating + 1)\n#     pred_hist = np.bincount(y_pred - min_rating, minlength=max_rating - min_rating + 1)\n#     E = np.outer(act_hist, pred_hist)\n#     E = E / E.sum()\n\n#     # Calculate QWK\n#     O = O / O.sum()\n#     num = np.sum(W * O)\n#     den = np.sum(W * E)\n#     return 1 - num / den\n\n# def compute_metrics_for_regression(eval_pred):\n#     \"\"\"\n#     Compute metrics for regression, including quadratic weighted kappa.\n#     eval_pred: A tuple (predictions, true_values)\n#     \"\"\"\n#     predictions, true_values = eval_pred\n#     predictions = np.rint(predictions).astype(int)  # Round to nearest integer for kappa\n#     true_values = np.rint(true_values).astype(int)  # Round to nearest integer for kappa\n\n#     qwk = quadratic_weighted_kappa(true_values, predictions)\n\n#     return {\n#         \"quadratic_weighted_kappa\": qwk\n#     }\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:45:06.473096Z","iopub.execute_input":"2024-05-20T02:45:06.473368Z","iopub.status.idle":"2024-05-20T02:45:06.483659Z","shell.execute_reply.started":"2024-05-20T02:45:06.473344Z","shell.execute_reply":"2024-05-20T02:45:06.482845Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nBERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nBERT_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n\ndef BERT_preprocess_function(examples):\n    return BERT_tokenizer(examples[\"text\"],  truncation=True)\n\nfor p in prompts:\n    print('PROMPT :', p)\n\n    train_df = load_data(p, 'train')\n    val_df = load_data(p, 'val')\n    test_df = load_data(p, 'test')\n\n    process_train = process(train_df,p)\n    process_val = process(val_df,p)\n    process_test = process(test_df,p)\n\n    datasets = DatasetDict({\n        \"train\": Dataset.from_pandas(process_train),\n        \"val\": Dataset.from_pandas(process_val),\n        \"test\": Dataset.from_pandas(process_test)\n    })\n\n    tokenized_datasets = datasets.map(BERT_preprocess_function, batched=True)\n    data_collator = DataCollatorWithPadding(tokenizer=BERT_tokenizer)\n    accuracy = evaluate.load(\"accuracy\") \n    \n    training_args = TrainingArguments(\n        output_dir=\"BERT_model\",\n        learning_rate=LEARNING_RATE,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=EPOCHS,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  \n        load_best_model_at_end=False,\n        \n    )\n    \n\n    trainer = RegressionTrainer(\n        model=BERT_model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"val\"],\n        tokenizer=BERT_tokenizer,\n        compute_metrics=compute_metrics_for_regression,\n        \n    )\n    trainer.train()\n    trainer.eval_dataset=tokenized_datasets[\"test\"]\n#     trainer.save_model('bert_model/prompt_{}'.format(p))\n    print(trainer.evaluate())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T03:36:58.599196Z","iopub.execute_input":"2024-05-20T03:36:58.599548Z","iopub.status.idle":"2024-05-20T04:13:57.515575Z","shell.execute_reply.started":"2024-05-20T03:36:58.599522Z","shell.execute_reply":"2024-05-20T04:13:57.513919Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PROMPT : 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c158376d464421cb9cb899913939695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee7c22591cfe4cd1a68e27b07b402f72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/179 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0f34f7c2af4551859fac2502b8afa7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='895' max='895' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [895/895 06:45, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.026265</td>\n      <td>0.026265</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.007217</td>\n      <td>0.007217</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.019700</td>\n      <td>0.008992</td>\n      <td>0.008992</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.019700</td>\n      <td>0.005551</td>\n      <td>0.005551</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.019700</td>\n      <td>0.006092</td>\n      <td>0.006092</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.006898785475641489, 'eval_mse': 0.006898785475641489, 'eval_runtime': 3.2367, 'eval_samples_per_second': 55.304, 'eval_steps_per_second': 7.106, 'epoch': 5.0}\nPROMPT : 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d55be4bad484f39adc8dafbc9febed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e37680abf6f04dd58ebdae7d7a2b9a5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677e6be58f574512b4c8285a10c310af"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 06:49, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.009458</td>\n      <td>0.009458</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.009290</td>\n      <td>0.009290</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.011700</td>\n      <td>0.010390</td>\n      <td>0.010390</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.011700</td>\n      <td>0.009852</td>\n      <td>0.009852</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.011700</td>\n      <td>0.010185</td>\n      <td>0.010185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.010720447637140751, 'eval_mse': 0.010720447637140751, 'eval_runtime': 3.2919, 'eval_samples_per_second': 54.679, 'eval_steps_per_second': 6.987, 'epoch': 5.0}\nPROMPT : 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1380 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc07e079900647a2a24b532d19c7c571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/173 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f4a14851174825b4aee4244642960f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/173 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7102bdf22c540e3968f2c4085c79666"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='865' max='865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [865/865 02:57, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.031595</td>\n      <td>0.031595</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.030301</td>\n      <td>0.030301</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.031900</td>\n      <td>0.034839</td>\n      <td>0.034839</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.031900</td>\n      <td>0.035585</td>\n      <td>0.035585</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.031900</td>\n      <td>0.035739</td>\n      <td>0.035739</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [22/22 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.04840991273522377, 'eval_mse': 0.04840992018580437, 'eval_runtime': 1.4646, 'eval_samples_per_second': 118.12, 'eval_steps_per_second': 15.021, 'epoch': 5.0}\nPROMPT : 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb9d908f51c422fba126231162a80fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/177 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d17ed72e3c442687d8367a25f670ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/177 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160c2d697219401b966e428d058480d4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='885' max='885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [885/885 02:53, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.036131</td>\n      <td>0.036131</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.037637</td>\n      <td>0.037637</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.026400</td>\n      <td>0.038581</td>\n      <td>0.038581</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.026400</td>\n      <td>0.034706</td>\n      <td>0.034706</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.026400</td>\n      <td>0.033891</td>\n      <td>0.033891</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.02407163381576538, 'eval_mse': 0.02407163940370083, 'eval_runtime': 1.4221, 'eval_samples_per_second': 124.46, 'eval_steps_per_second': 16.173, 'epoch': 5.0}\nPROMPT : 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1444 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b2c68ede3445d686d7d4af0c786a98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f32f3028a1314ae2954422125f80973e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/181 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6501a423bd754923a6ffa1fc18e73200"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='905' max='905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [905/905 03:35, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.016360</td>\n      <td>0.016360</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.016154</td>\n      <td>0.016154</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.015400</td>\n      <td>0.015729</td>\n      <td>0.015729</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.015400</td>\n      <td>0.015591</td>\n      <td>0.015591</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.015400</td>\n      <td>0.015549</td>\n      <td>0.015549</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.014837196096777916, 'eval_mse': 0.01483719889074564, 'eval_runtime': 1.6797, 'eval_samples_per_second': 107.756, 'eval_steps_per_second': 13.693, 'epoch': 5.0}\nPROMPT : 6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a7f04da6f424831b0c2ddfa35f174d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ad87c8db8442b58d62b9f05a6f7047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"800d8db3474f4670a9bef5414ceb5504"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 04:02, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.017150</td>\n      <td>0.017150</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.016126</td>\n      <td>0.016126</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.016300</td>\n      <td>0.014219</td>\n      <td>0.014219</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.016300</td>\n      <td>0.013682</td>\n      <td>0.013682</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.016300</td>\n      <td>0.014118</td>\n      <td>0.014118</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.01959497667849064, 'eval_mse': 0.019594978541135788, 'eval_runtime': 1.9762, 'eval_samples_per_second': 91.086, 'eval_steps_per_second': 11.639, 'epoch': 5.0}\nPROMPT : 7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1255 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d085acbc047d4ddfaaf5b7e0bda524a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0b9bb2abfc34563b4c5bf48953f57da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecf8b72c64c40d3bae279c48ade7a44"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='785' max='785' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [785/785 04:37, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.006402</td>\n      <td>0.006402</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.005933</td>\n      <td>0.005933</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.006296</td>\n      <td>0.006296</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.006600</td>\n      <td>0.006569</td>\n      <td>0.006569</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.006600</td>\n      <td>0.006438</td>\n      <td>0.006438</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.007680067792534828, 'eval_mse': 0.007680065929889679, 'eval_runtime': 2.3428, 'eval_samples_per_second': 67.015, 'eval_steps_per_second': 8.537, 'epoch': 5.0}\nPROMPT : 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/578 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d179115a024457be5ff8a6dbb91c22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/72 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44bb89708856456085231fdca37eafb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/73 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1652fc07429244a79a59768194eb0e8a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='365' max='365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [365/365 02:44, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.003802</td>\n      <td>0.003802</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.004007</td>\n      <td>0.004007</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.003758</td>\n      <td>0.003758</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.004110</td>\n      <td>0.004110</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.003799</td>\n      <td>0.003799</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.003353484906256199, 'eval_mse': 0.0033534851390868425, 'eval_runtime': 1.3418, 'eval_samples_per_second': 54.406, 'eval_steps_per_second': 7.453, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# T5","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\nT5_tokenizer = AutoTokenizer.from_pretrained('t5-small')\nT5_model = T5ForConditionalGeneration.from_pretrained('t5-small', num_labels=1)\n\ndef T5_preprocess_function(examples):\n    return T5_tokenizer(examples[\"text\"],  truncation=True)\n\n\nfor p in prompts:\n    print('PROMPT :', p)\n\n    train_df = load_data(p, 'train')\n    val_df = load_data(p, 'val')\n    test_df = load_data(p, 'test')\n    \n    process_train = process(train_df,p)\n    process_val = process(val_df,p)\n    process_test = process(test_df,p)\n    \n    #print(process_train.label.unique())\n\n    datasets = DatasetDict({\n        \"train\": Dataset.from_pandas(process_train),\n        \"val\": Dataset.from_pandas(process_val),\n        \"test\": Dataset.from_pandas(process_test)\n    })\n\n    tokenized_datasets = datasets.map(T5_preprocess_function, batched=True)\n    #print(tokenized_datasets[\"train\"][0])\n    data_collator = DataCollatorWithPadding(tokenizer=T5_tokenizer)\n    accuracy = evaluate.load(\"accuracy\") #name??\n    \n    training_args = TrainingArguments(\n        output_dir=\"T5_model\",\n        learning_rate=LEARNING_RATE,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=EPOCHS,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  \n        load_best_model_at_end=False,\n    )\n\n    trainer = RegressionTrainer(\n        model=T5_model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"val\"],\n        tokenizer=T5_tokenizer,\n        compute_metrics=compute_metrics_for_regression,\n    )\n\n    trainer.train()\n    trainer.eval_dataset=tokenized_datasets[\"test\"]\n    print(trainer.evaluate())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:45:23.023160Z","iopub.status.idle":"2024-05-20T02:45:23.024429Z","shell.execute_reply.started":"2024-05-20T02:45:23.024188Z","shell.execute_reply":"2024-05-20T02:45:23.024208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLM","metadata":{}},{"cell_type":"code","source":"pip install sacremoses","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:45:23.026160Z","iopub.status.idle":"2024-05-20T02:45:23.026959Z","shell.execute_reply.started":"2024-05-20T02:45:23.026699Z","shell.execute_reply":"2024-05-20T02:45:23.026721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nXLM_tokenizer = AutoTokenizer.from_pretrained('xlm-mlm-en-2048')\nXLM_model = AutoModelForSequenceClassification.from_pretrained('xlm-mlm-en-2048', num_labels=1)\n\ndef XLM_preprocess_function(examples):\n    return XLM_tokenizer(examples[\"text\"],  truncation=True)\n\nfor p in prompts:\n    print('PROMPT :', p)\n\n    train_df = load_data(p, 'train')\n    val_df = load_data(p, 'val')\n    test_df = load_data(p, 'test')\n\n    process_train = process(train_df,p)\n    process_val = process(val_df,p)\n    process_test = process(test_df,p)\n\n    #print(process_train.label.unique())\n\n    datasets = DatasetDict({\n        \"train\": Dataset.from_pandas(process_train),\n        \"val\": Dataset.from_pandas(process_val),\n        \"test\": Dataset.from_pandas(process_test)\n    })\n\n    tokenized_datasets = datasets.map(XLM_preprocess_function, batched=True)\n    #print(tokenized_datasets[\"train\"][0])\n    data_collator = DataCollatorWithPadding(tokenizer=XLM_tokenizer)\n    accuracy = evaluate.load(\"accuracy\") #name??\n    \n    training_args = TrainingArguments(\n        output_dir=\"XLM_model\",\n        learning_rate=LEARNING_RATE,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=EPOCHS,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  \n        load_best_model_at_end=False,\n    )\n\n    trainer = RegressionTrainer(\n        model=XLM_model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"val\"],\n        tokenizer=XLM_tokenizer,\n        compute_metrics=compute_metrics_for_regression,\n    )\n\n    trainer.train()\n    trainer.eval_dataset=tokenized_datasets[\"test\"]\n    print(trainer.evaluate())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:45:23.028285Z","iopub.status.idle":"2024-05-20T02:45:23.028748Z","shell.execute_reply.started":"2024-05-20T02:45:23.028498Z","shell.execute_reply":"2024-05-20T02:45:23.028516Z"},"trusted":true},"execution_count":null,"outputs":[]}]}